{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Finetune Florence-2 Model for Object Detection\n",
    "Florence-2 has a great capability of detecting various objects in a zero-shot setting with the task prompt \"<OD>\". \n",
    "However, if you want to detect specific objects that the base model is not able to in its current form, this notebook shows how you can finetune the model to perform this task with your custom data. \n",
    "Here I show how to finetune the model to detect tables in a given image, but a similar process can be applied to detect any objects. \n",
    "For finetuning Florence-2 in VQA from documents, please check HuggingFace Florence-2 Finetuning blog post and for inference Florence-2-Large Sample Inference. \n",
    "This notebook is heavily inspired by them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U git+https://github.com/huggingface/transformers.git accelerate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flash_attn timm einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94b6dcfec1a4b048a97947618ab8f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbab9cb099ab4cbe885b2079721882af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/225M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0c4d0b1fa945b19663349634234145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ucsahin/pubtables-detection-1500-samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=771x1000>,\n",
       " 'objects': {'bbox': [[75.53573789752897,\n",
       "    805.4076686049953,\n",
       "    580.396252241291,\n",
       "    882.670855281329]],\n",
       "  'categories': 'table'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def preprocess_fnc(examples):\n",
    "    bbox_formatted_list = []\n",
    "    for objects, image in zip(examples[\"objects\"], examples[\"image\"]):\n",
    "        width, height = image.size\n",
    "        bins_w, bins_h = [1000, 1000]  # Quantization bins.\n",
    "        size_per_bin_w = width / bins_w\n",
    "        size_per_bin_h = height / bins_h\n",
    "\n",
    "        bboxes = objects[\"bbox\"]\n",
    "        bbox_str = \"\"\n",
    "        for bbox in bboxes:\n",
    "            # if you are to detect multiple objects, bbox_str should contain category labels before the bounding boxes\n",
    "            if len(bbox_str) == 0:\n",
    "                bbox_str += \"table\"\n",
    "            bbox = bbox.copy()\n",
    "\n",
    "            xmin, ymin, xmax, ymax = torch.tensor(bbox).split(1, dim=-1)\n",
    "            quantized_xmin = (\n",
    "                xmin / size_per_bin_w).floor().clamp(0, bins_w - 1)\n",
    "            quantized_ymin = (\n",
    "                ymin / size_per_bin_h).floor().clamp(0, bins_h - 1)\n",
    "            quantized_xmax = (\n",
    "                xmax / size_per_bin_w).floor().clamp(0, bins_w - 1)\n",
    "            quantized_ymax = (\n",
    "                ymax / size_per_bin_h).floor().clamp(0, bins_h - 1)\n",
    "\n",
    "            quantized_boxes = torch.cat(\n",
    "                (quantized_xmin, quantized_ymin, quantized_xmax, quantized_ymax), dim=-1\n",
    "            ).int()\n",
    "\n",
    "            bbox_str += f\"<loc_{quantized_boxes[0]}><loc_{quantized_boxes[1]}><loc_{quantized_boxes[2]}><loc_{quantized_boxes[3]}>\"\n",
    "\n",
    "        bbox_formatted_list.append(bbox_str)\n",
    "\n",
    "    examples[\"bbox_str\"] = bbox_formatted_list\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819a7cbf449d41a3a5d5e88fc85924b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_dataset = dataset.map(preprocess_fnc, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len train dataset:  1350\n",
      "Len eval dataset:  150\n"
     ]
    }
   ],
   "source": [
    "split_dataset = processed_dataset[\"train\"].train_test_split(test_size=0.1, shuffle=True)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(\"Len train dataset: \", len(train_dataset))\n",
    "print(\"Len eval dataset: \", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large-ft\", revision=\"refs/pr/10\", trust_remote_code=True) # load the model on GPU\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large-ft\", revision=\"refs/pr/10\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example(task_prompt, image, max_new_tokens=128):\n",
    "    prompt = task_prompt\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    generated_ids = model.generate(\n",
    "      input_ids=inputs[\"input_ids\"],\n",
    "      pixel_values=inputs[\"pixel_values\"],\n",
    "      max_new_tokens=max_new_tokens,\n",
    "      early_stopping=False,\n",
    "      do_sample=False,\n",
    "      num_beams=3,\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    parsed_answer = processor.post_process_generation(\n",
    "        generated_text,\n",
    "        task=task_prompt,\n",
    "        image_size=(image.width, image.height)\n",
    "    )\n",
    "    return parsed_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def plot_bbox(image, data):\n",
    "   # Create a figure and axes\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Plot each bounding box\n",
    "    for bbox, label in zip(data['bboxes'], data['labels']):\n",
    "        # Unpack the bounding box coordinates\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        # Add the rectangle to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        # Annotate the label\n",
    "        plt.text(x1, y1, label, color='white', fontsize=8, bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    # Remove the axis ticks and labels\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_id = 250\n",
    "image = processed_dataset[\"train\"][example_id][\"image\"]\n",
    "\n",
    "# notice here that <OD> task prompt is used. This task prompt is already used in training the Florence-2 model checkpoints for object detection.\n",
    "parsed_answer = run_example(task_prompt=\"<OD>\", image=image)\n",
    "\n",
    "plot_bbox(image, parsed_answer[\"<OD>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.vision_tower.parameters():\n",
    "  param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters 462061568 out of 822693888, rate: 0.562\n"
     ]
    }
   ],
   "source": [
    "model_total_params = sum(p.numel() for p in model.parameters())\n",
    "model_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Number of trainable parameters {model_train_params} out of {model_total_params}, rate: {model_train_params/model_total_params:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_ID = -100 # Pytorch ignore index when computing loss\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "def collate_fn(examples):\n",
    "    task_prompt = \"<OD>\"\n",
    "\n",
    "    prompt_texts = [task_prompt for _ in examples]\n",
    "    label_texts = [example[\"bbox_str\"] for example in examples]\n",
    "    images = [example[\"image\"] for example in examples]\n",
    "\n",
    "    inputs = processor(\n",
    "        images=images,\n",
    "        text=prompt_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "    labels = processor.tokenizer(\n",
    "        label_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_token_type_ids=False, # no need to set this to True since BART does not use token type ids\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = IGNORE_ID # do not learn to predict pad tokens during training\n",
    "\n",
    "    return_data = {**inputs, \"labels\": labels}\n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "collated_examples = collate_fn([train_dataset[0], train_dataset[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collated_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args=TrainingArguments(\n",
    "    output_dir=\"./ft_models/Florence-2-large-TableDetection\",\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_total_limit=5,\n",
    "    load_best_model_at_end=False, # we will manually push model to the hub at the end of training\n",
    "    label_names=[\"labels\"],\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False,  # needed for data collator\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"ucsahin/Florence-2-large-TableDetection\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or install tensorboardX.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[0;32m----> 3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# dont forget to add custom data collator\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/auslander_test/.venv/lib/python3.11/site-packages/transformers/trainer.py:578\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    576\u001b[0m default_callbacks \u001b[38;5;241m=\u001b[39m DEFAULT_CALLBACKS \u001b[38;5;241m+\u001b[39m get_reporting_integration_callbacks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mreport_to)\n\u001b[1;32m    577\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m default_callbacks \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m default_callbacks \u001b[38;5;241m+\u001b[39m callbacks\n\u001b[0;32m--> 578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler \u001b[38;5;241m=\u001b[39m \u001b[43mCallbackHandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_callback(PrinterCallback \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdisable_tqdm \u001b[38;5;28;01melse\u001b[39;00m DEFAULT_PROGRESS_CALLBACK)\n\u001b[1;32m    583\u001b[0m \u001b[38;5;66;03m# Will be set to True by `self._setup_loggers()` on first call to `self.log()`.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/github/auslander_test/.venv/lib/python3.11/site-packages/transformers/trainer_callback.py:404\u001b[0m, in \u001b[0;36mCallbackHandler.__init__\u001b[0;34m(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks:\n\u001b[0;32m--> 404\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n",
      "File \u001b[0;32m~/Documents/github/auslander_test/.venv/lib/python3.11/site-packages/transformers/trainer_callback.py:421\u001b[0m, in \u001b[0;36mCallbackHandler.add_callback\u001b[0;34m(self, callback)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_callback\u001b[39m(\u001b[38;5;28mself\u001b[39m, callback):\n\u001b[0;32m--> 421\u001b[0m     cb \u001b[38;5;241m=\u001b[39m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callback, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m callback\n\u001b[1;32m    422\u001b[0m     cb_class \u001b[38;5;241m=\u001b[39m callback \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callback, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m callback\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cb_class \u001b[38;5;129;01min\u001b[39;00m [c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks]:\n",
      "File \u001b[0;32m~/Documents/github/auslander_test/.venv/lib/python3.11/site-packages/transformers/integrations/integration_utils.py:638\u001b[0m, in \u001b[0;36mTensorBoardCallback.__init__\u001b[0;34m(self, tb_writer)\u001b[0m\n\u001b[1;32m    636\u001b[0m has_tensorboard \u001b[38;5;241m=\u001b[39m is_tensorboard_available()\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_tensorboard:\n\u001b[0;32m--> 638\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    640\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m install tensorboardX.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    641\u001b[0m     )\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_tensorboard:\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or install tensorboardX."
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=processor,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn, # dont forget to add custom data collator\n",
    "    args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
